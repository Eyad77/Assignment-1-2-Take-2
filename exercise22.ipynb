{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 2: Logistic Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, you will implement logistic regression and apply it to two different datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and\n",
    "you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions\n",
    "decision. Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. \n",
    "\n",
    "The following cell will load the data and corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# The first two columns contains the exam scores and the third column\n",
    "# contains the label.\n",
    "data = np.loadtxt(os.path.join('Data', 'ex2data1.txt'), delimiter=',')\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X, y = data[:, 0:2], data[:, 2]\n",
    "\n",
    "# Setup the data matrix appropriately, and add ones for the intercept term\n",
    "\n",
    "# Add intercept term to X\n",
    "X = np.concatenate([np.ones((data.shape[0], 1)), X], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = np.zeros((X.shape[0],5))\n",
    "new_X[:,0:3] = X[:,0:3]\n",
    "new_X[:,3] = X[:,1]**2\n",
    "new_X[:,4] = X[:,2]**3\n",
    "\n",
    "\n",
    "# 60% training set \n",
    "X_train = new_X[0:60,:]\n",
    "y_train = y[0:60]\n",
    "\n",
    "# 20% cross validation set\n",
    "X_cross = new_X[60:80,:]\n",
    "y_cross = y[60:80]\n",
    "\n",
    "# 20% testing set\n",
    "X_test = new_X[80:100,:]\n",
    "y_test = y[80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 5.04581598e+01, 7.58098595e+01, 2.54602589e+03,\n",
       "        4.35689482e+05],\n",
       "       [1.00000000e+00, 8.22266616e+01, 4.27198785e+01, 6.76122387e+03,\n",
       "        7.79632666e+04],\n",
       "       [1.00000000e+00, 9.92725269e+01, 6.09990310e+01, 9.85503460e+03,\n",
       "        2.26970183e+05],\n",
       "       [1.00000000e+00, 3.58474088e+01, 7.29021980e+01, 1.28503672e+03,\n",
       "        3.87455534e+05],\n",
       "       [1.00000000e+00, 7.47892530e+01, 4.15734152e+01, 5.59343236e+03,\n",
       "        7.18533646e+04],\n",
       "       [1.00000000e+00, 8.89138964e+01, 6.98037889e+01, 7.90568098e+03,\n",
       "        3.40123774e+05],\n",
       "       [1.00000000e+00, 4.04575510e+01, 9.75351855e+01, 1.63681343e+03,\n",
       "        9.27863183e+05],\n",
       "       [1.00000000e+00, 5.39710521e+01, 8.92073501e+01, 2.91287447e+03,\n",
       "        7.09907750e+05],\n",
       "       [1.00000000e+00, 6.18302060e+01, 5.02561079e+01, 3.82297438e+03,\n",
       "        1.26930665e+05],\n",
       "       [1.00000000e+00, 6.40393204e+01, 7.80316880e+01, 4.10103456e+03,\n",
       "        4.75130605e+05],\n",
       "       [1.00000000e+00, 5.53400176e+01, 6.49319380e+01, 3.06251754e+03,\n",
       "        2.73763217e+05],\n",
       "       [1.00000000e+00, 6.67467186e+01, 6.09913940e+01, 4.45512444e+03,\n",
       "        2.26884945e+05],\n",
       "       [1.00000000e+00, 4.72642691e+01, 8.84758650e+01, 2.23391113e+03,\n",
       "        6.92587185e+05],\n",
       "       [1.00000000e+00, 7.50247456e+01, 4.65540135e+01, 5.62871245e+03,\n",
       "        1.00895404e+05],\n",
       "       [1.00000000e+00, 6.20730638e+01, 9.67688241e+01, 3.85306525e+03,\n",
       "        9.06163138e+05],\n",
       "       [1.00000000e+00, 8.44328200e+01, 4.35333933e+01, 7.12890109e+03,\n",
       "        8.25025860e+04],\n",
       "       [1.00000000e+00, 6.93645888e+01, 9.77186920e+01, 4.81144617e+03,\n",
       "        9.33110196e+05],\n",
       "       [1.00000000e+00, 7.23464942e+01, 9.62275930e+01, 5.23401523e+03,\n",
       "        8.91043420e+05],\n",
       "       [1.00000000e+00, 5.72387063e+01, 5.95142820e+01, 3.27626950e+03,\n",
       "        2.10796597e+05],\n",
       "       [1.00000000e+00, 7.50136584e+01, 3.06032632e+01, 5.62704894e+03,\n",
       "        2.86617837e+04],\n",
       "       [1.00000000e+00, 8.03667560e+01, 9.09601479e+01, 6.45881547e+03,\n",
       "        7.52581388e+05],\n",
       "       [1.00000000e+00, 5.15477203e+01, 4.68562903e+01, 2.65716746e+03,\n",
       "        1.02873545e+05],\n",
       "       [1.00000000e+00, 8.98458067e+01, 4.53582836e+01, 8.07226898e+03,\n",
       "        9.33189485e+04],\n",
       "       [1.00000000e+00, 7.99448179e+01, 7.41631194e+01, 6.39117392e+03,\n",
       "        4.07909636e+05],\n",
       "       [1.00000000e+00, 7.54777020e+01, 9.04245390e+01, 5.69688350e+03,\n",
       "        7.39365037e+05],\n",
       "       [1.00000000e+00, 7.47758930e+01, 8.95298129e+01, 5.59143417e+03,\n",
       "        7.17634040e+05],\n",
       "       [1.00000000e+00, 5.54821611e+01, 3.55707035e+01, 3.07827020e+03,\n",
       "        4.50067199e+04],\n",
       "       [1.00000000e+00, 3.52861128e+01, 4.70205139e+01, 1.24510976e+03,\n",
       "        1.03959005e+05],\n",
       "       [1.00000000e+00, 9.48345067e+01, 4.56943068e+01, 8.99358367e+03,\n",
       "        9.54083269e+04],\n",
       "       [1.00000000e+00, 4.90725632e+01, 5.18832118e+01, 2.40811646e+03,\n",
       "        1.39662740e+05],\n",
       "       [1.00000000e+00, 6.79468555e+01, 4.66785741e+01, 4.61677517e+03,\n",
       "        1.01707445e+05],\n",
       "       [1.00000000e+00, 4.50832775e+01, 5.63163718e+01, 2.03250191e+03,\n",
       "        1.78609273e+05],\n",
       "       [1.00000000e+00, 5.21079797e+01, 6.31276238e+01, 2.71524155e+03,\n",
       "        2.51569697e+05],\n",
       "       [1.00000000e+00, 5.05347883e+01, 4.88558115e+01, 2.55376483e+03,\n",
       "        1.16613464e+05],\n",
       "       [1.00000000e+00, 9.98278578e+01, 7.23692519e+01, 9.96560119e+03,\n",
       "        3.79020107e+05],\n",
       "       [1.00000000e+00, 7.69787837e+01, 4.75759636e+01, 5.92573314e+03,\n",
       "        1.07686877e+05],\n",
       "       [1.00000000e+00, 6.22226758e+01, 5.20609919e+01, 3.87166138e+03,\n",
       "        1.41103347e+05],\n",
       "       [1.00000000e+00, 4.20754545e+01, 7.88447860e+01, 1.77034387e+03,\n",
       "        4.90138634e+05],\n",
       "       [1.00000000e+00, 4.46682617e+01, 6.64500861e+01, 1.99525361e+03,\n",
       "        2.93417927e+05],\n",
       "       [1.00000000e+00, 6.90701441e+01, 5.27404697e+01, 4.77068480e+03,\n",
       "        1.46700631e+05],\n",
       "       [1.00000000e+00, 4.22617008e+01, 8.71038509e+01, 1.78605136e+03,\n",
       "        6.60863959e+05],\n",
       "       [1.00000000e+00, 3.39155001e+01, 9.88694357e+01, 1.15026115e+03,\n",
       "        9.66465080e+05],\n",
       "       [1.00000000e+00, 9.76456340e+01, 6.88615727e+01, 9.53466983e+03,\n",
       "        3.26535807e+05],\n",
       "       [1.00000000e+00, 6.01825994e+01, 8.63085521e+01, 3.62194527e+03,\n",
       "        6.42926746e+05],\n",
       "       [1.00000000e+00, 8.96767758e+01, 6.57993659e+01, 8.04192411e+03,\n",
       "        2.84882076e+05],\n",
       "       [1.00000000e+00, 7.60987867e+01, 8.74205697e+01, 5.79102534e+03,\n",
       "        6.68099116e+05],\n",
       "       [1.00000000e+00, 8.34891627e+01, 4.83802858e+01, 6.97044030e+03,\n",
       "        1.13241415e+05],\n",
       "       [1.00000000e+00, 5.88409562e+01, 7.58584483e+01, 3.46225813e+03,\n",
       "        4.36527758e+05],\n",
       "       [1.00000000e+00, 7.44926924e+01, 8.48451368e+01, 5.54916122e+03,\n",
       "        6.10774453e+05],\n",
       "       [1.00000000e+00, 3.41836400e+01, 7.52377203e+01, 1.16852125e+03,\n",
       "        4.25899259e+05],\n",
       "       [1.00000000e+00, 6.65608945e+01, 4.10920981e+01, 4.43035267e+03,\n",
       "        6.93864947e+04],\n",
       "       [1.00000000e+00, 8.39023937e+01, 5.63080462e+01, 7.03961166e+03,\n",
       "        1.78530070e+05],\n",
       "       [1.00000000e+00, 3.42120610e+01, 4.42095286e+01, 1.17046512e+03,\n",
       "        8.64067464e+04],\n",
       "       [1.00000000e+00, 7.53956115e+01, 8.57599367e+01, 5.68449823e+03,\n",
       "        6.30744330e+05],\n",
       "       [1.00000000e+00, 8.23687538e+01, 4.06182552e+01, 6.78461160e+03,\n",
       "        6.70137298e+04],\n",
       "       [1.00000000e+00, 6.04578857e+01, 7.30949981e+01, 3.65515595e+03,\n",
       "        3.90537712e+05],\n",
       "       [1.00000000e+00, 9.44433678e+01, 6.55689216e+01, 8.91954972e+03,\n",
       "        2.81899381e+05],\n",
       "       [1.00000000e+00, 3.87858038e+01, 6.49956810e+01, 1.50433858e+03,\n",
       "        2.74570260e+05],\n",
       "       [1.00000000e+00, 8.54045194e+01, 5.70519840e+01, 7.29393193e+03,\n",
       "        1.85700150e+05],\n",
       "       [1.00000000e+00, 7.06615096e+01, 9.29271379e+01, 4.99304893e+03,\n",
       "        8.02467928e+05]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. This can be a 1-D vector \n",
    "        or a 2-D matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z)\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        The parameters for logistic regression. This a vector\n",
    "        of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1) where m is the total number\n",
    "        of data points and n is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    y : arra_like\n",
    "        Labels for the input. This is a vector of shape (m, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n+1, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to \n",
    "    the cost. Compute the partial derivatives and set grad to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
    "    grad = (1 / m) * (h - y).dot(X)\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(theta,X,y):\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done call your `costFunction` using two test cases for  $\\theta$ by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta found by optimize.minimize: 0.204\n",
      "theta:\n",
      "\t[-23.135, 0.195, 0.186]\n"
     ]
    }
   ],
   "source": [
    "# set options for optimize.minimize\n",
    "options= {'maxiter': 400}\n",
    "\n",
    "# see documention for scipy's optimize.minimize  for description about\n",
    "# the different parameters\n",
    "# The function returns an object `OptimizeResult`\n",
    "# We use truncated Newton algorithm for optimization which is \n",
    "# equivalent to MATLAB's fminunc\n",
    "# See https://stackoverflow.com/questions/18801002/fminunc-alternate-in-numpy\n",
    "\n",
    "\n",
    "\n",
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(3)\n",
    "\n",
    "\n",
    "\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_theta,\n",
    "                        (X_train[:,0:3], y_train),\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# the fun property of `OptimizeResult` object returns\n",
    "# the value of costFunction at optimized theta\n",
    "cost = res.fun\n",
    "\n",
    "# the optimized theta is in the x property\n",
    "theta = res.x\n",
    "\n",
    "# Print theta to screen\n",
    "print('Cost at theta found by optimize.minimize: {:.3f}'.format(cost))\n",
    "\n",
    "print('theta:')\n",
    "print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression.\n",
    "    Computes the predictions for X using a threshold at 0.5 \n",
    "    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Parameters for logistic regression. A vecotor of shape (n+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The data to use for computing predictions. The rows is the number \n",
    "        of points to compute predictions, and columns is the number of\n",
    "        features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        Predictions and 0 or 1 for each row in X. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned \n",
    "    logistic regression parameters.You should set p to a vector of 0's and 1's    \n",
    "    \"\"\"\n",
    "    m = X.shape[0] # Number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    p = np.round(sigmoid(X.dot(theta.T)))\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code in `predict`, we proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute accuracy on our training set\n",
    "p = predict(theta, X_test[:,0:4])\n",
    "print('Train Accuracy: {:.2f} %'.format(np.mean(p == y_test) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      "[1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(p[0:11])\n",
    "print(y_test[0:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Regularized logistic regression\n",
    "\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n",
    "\n",
    "First, we load the data from a CSV file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 2.3 Cost function and gradient\n",
    "\n",
    "Now you will implement code to compute the cost function and gradient for regularized logistic regression. Complete the code for the function `costFunctionReg` below to return the cost and gradient.\n",
    "\n",
    "Recall that the regularized cost function in logistic regression is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\theta \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "\n",
    "Note that you should not regularize the parameters $\\theta_0$. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\qquad \\text{for } j \\ge 1 $$\n",
    "<a id=\"costFunctionReg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Logistic regression parameters. A vector with shape (n, ). n is \n",
    "        the number of features including any intercept. If we have mapped\n",
    "        our initial features into polynomial features, then n is the total \n",
    "        number of polynomial features. \n",
    "    \n",
    "    X : array_like\n",
    "        The data set with shape (m x n). m is the number of examples, and\n",
    "        n is the number of features (after feature mapping).\n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector with shape (m, ).\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost `J` of a particular choice of theta.\n",
    "    Compute the partial derivatives and set `grad` to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    temp = theta\n",
    "    temp[0] = 0\n",
    "    \n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n",
    "    \n",
    "    grad = (1 / m) * (h - y).dot(X) \n",
    "    grad = grad + (lambda_ / m) * temp\n",
    "    # =============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostReg(theta,X,y,lambda_):\n",
    "    # Initialize some useful values\n",
    "    m = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    \n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(theta))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is equal =  0.3097337573003858\n",
      "cost is equal =  0.3109055290958805\n",
      "cost is equal =  0.31341241778878737\n",
      "cost is equal =  0.31657023747496843\n",
      "cost is equal =  0.31329802257101497\n",
      "cost is equal =  0.32443726581422067\n",
      "cost is equal =  0.3283905505620721\n",
      "cost is equal =  0.335014127467089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set regularization parameter lambda to 1 (you should vary this)\n",
    "lambda_ = [0,0.01,0.05,0.1,0.5,1,5,10]\n",
    "# set options for optimize.minimize\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "for i in range(len(lambda_)):\n",
    "    \n",
    "    # Initialize fitting parameters\n",
    "    initial_theta = np.zeros(5)\n",
    "    \n",
    "    res = optimize.minimize(costFunctionReg,\n",
    "                            initial_theta,\n",
    "                            (X_train[:,0:5], y_train, lambda_[i]),\n",
    "                            jac=True,\n",
    "                            method='TNC',\n",
    "                            options=options)\n",
    "    print('cost is equal = ',computeCostReg(res.x,X_cross[:,0:5],y_cross,lambda_[i]))\n",
    "\n",
    "\n",
    "# the optimized theta is in the x property of the result\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
